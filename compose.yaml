services:
  llm_proxy:
    container_name: llm_proxy 
    build: ./llm_proxy
    ports:
      - "8000:8000"
    # command: sleep infinity for debugging
    command: sh -c "poetry install --no-root && poetry run fastapi run app/main.py"
    working_dir: /workspace/llm_proxy/
    volumes:
      # Mount the root folder that contains .git
      - .:/workspace:cached

  ollama:
    volumes:
      - ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    restart: unless-stopped
    image: ollama/ollama:0.5.7
    expose:
      - "11434"  # Expose Ollama on the default port 
    post_start:
      - command: ollama pull llama3.1:8b 
        user: root

volumes:
  ollama: {}
