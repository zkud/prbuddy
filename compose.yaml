services:
  llm_proxy:
    container_name: llm_proxy 
    build: ./llm_proxy
    command: sleep infinity
    volumes:
      # Mount the root folder that contains .git
      - .:/workspace:cached

  ollama:
    volumes:
      - ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    restart: unless-stopped
    image: ollama/ollama:0.5.7
    ports:
      - "11434:11434"  # Expose Ollama on the default port 
    post_start:
      - command: ollama pull llama3.1:8b 
        user: root

volumes:
  ollama: {}
